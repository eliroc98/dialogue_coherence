{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import transformers as ppb\n",
    "import json\n",
    "from transformers import AutoTokenizer, AutoModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# bert-base-cased-conversational"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at DeepPavlov/bert-base-cased-conversational were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"DeepPavlov/bert-base-cased-conversational\")\n",
    "model = AutoModel.from_pretrained(\"DeepPavlov/bert-base-cased-conversational\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dialoghi interi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13118/13118 [00:05<00:00, 2288.96it/s]\n"
     ]
    }
   ],
   "source": [
    "lst_dict = []\n",
    "with open('/Users/lizzy/Desktop/Universita/tesi/git/dialogue_coherence/data/EMNLP_dataset/dialogues_text.txt', 'r') as f:\n",
    "    lines = f.readlines()\n",
    "    for l in tqdm(lines):\n",
    "        tokenized = tokenizer.encode(l.replace(\"__eou__\", \"[SEP]\"))[:-1]\n",
    "        if len(tokenized) < 513:\n",
    "            lst_dict.append({'tokens': tokenizer.convert_ids_to_tokens(tokenized), \n",
    "                             'words': [\"[CLS]\"] + l.replace(\"__eou__\", \"[SEP]\").replace(\"'\", \" ' \").split(), \n",
    "                             'text': l.replace(\"__eou__\", \"\").replace(\"\\n\", \"\")})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Coppie di utterance nel dialogo.\\\n",
    "Es:\n",
    "- 1>1 ; 1>2 ; 1>3\n",
    "- 2>2 ; 2>3\n",
    "- 3>3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13118/13118 [04:54<00:00, 44.48it/s] \n"
     ]
    }
   ],
   "source": [
    "lst_dict = []\n",
    "with open('/Users/lizzy/Desktop/Universita/tesi/git/dialogue_coherence/data/EMNLP_dataset/dialogues_text.txt', 'r') as f:\n",
    "    lines = f.readlines()\n",
    "    for l in tqdm(lines):\n",
    "        utt_lst = l.split(\"__eou__\")\n",
    "        for i, utt1 in enumerate(utt_lst):\n",
    "            for utt2 in utt_lst[i:]:\n",
    "                tokenized = tokenizer.encode(utt1+\" [SEP] \"+utt2)\n",
    "                utt1 = utt1.replace(\"'\", \" ' \").replace(\"\\n\", \"\")\n",
    "                utt2 = utt2.replace(\"'\", \" ' \").replace(\"\\n\", \"\")\n",
    "                lst_dict.append({'tokens': tokenizer.convert_ids_to_tokens(tokenized), \n",
    "                                    'words': [\"[CLS]\"] + utt1.split() + [\"[SEP]\"] + utt2.split() + [\"[SEP]\"], \n",
    "                                    'text': utt1 + \" \" + utt2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "str_to_save = json.dumps(lst_dict[:50])\n",
    "with open('/Users/lizzy/Desktop/Universita/tesi/git/dialogue_coherence/preprocessed_conversational.json', 'w') as f:\n",
    "       f.write(str_to_save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Creating examples...',\n",
       " 'WARNING:tensorflow:From /Users/lizzy/Desktop/Universita/tesi/git/dialogue_coherence/venv/lib/python3.9/site-packages/tensorflow/python/compat/v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.',\n",
       " 'Instructions for updating:',\n",
       " 'non-resource variables are not supported in the long term',\n",
       " 'Building BERT model...',\n",
       " '/Users/lizzy/Desktop/Universita/tesi/git/dialogue_coherence/bert/modeling.py:692: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.',\n",
       " '  query_layer = tfv1.layers.dense(',\n",
       " '/Users/lizzy/Desktop/Universita/tesi/git/dialogue_coherence/venv/lib/python3.9/site-packages/keras/legacy_tf_layers/core.py:261: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.',\n",
       " '  return layer.apply(inputs)',\n",
       " '/Users/lizzy/Desktop/Universita/tesi/git/dialogue_coherence/bert/modeling.py:701: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.',\n",
       " '  key_layer = tfv1.layers.dense(',\n",
       " '/Users/lizzy/Desktop/Universita/tesi/git/dialogue_coherence/bert/modeling.py:710: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.',\n",
       " '  value_layer = tfv1.layers.dense(',\n",
       " '/Users/lizzy/Desktop/Universita/tesi/git/dialogue_coherence/bert/modeling.py:894: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.',\n",
       " '  attention_output = tfv1.layers.dense(',\n",
       " '/Users/lizzy/Desktop/Universita/tesi/git/dialogue_coherence/bert/modeling.py:905: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.',\n",
       " '  intermediate_output = tfv1.layers.dense(',\n",
       " '/Users/lizzy/Desktop/Universita/tesi/git/dialogue_coherence/bert/modeling.py:915: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.',\n",
       " '  layer_output = tfv1.layers.dense(',\n",
       " '/Users/lizzy/Desktop/Universita/tesi/git/dialogue_coherence/bert/modeling.py:239: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.',\n",
       " '  self.pooled_output = tfv1.layers.dense(',\n",
       " 'Loading BERT from checkpoint...',\n",
       " 'Extracting attention maps...',\n",
       " 'Metal device set to: Apple M1',\n",
       " '',\n",
       " 'systemMemory: 16.00 GB',\n",
       " 'maxCacheSize: 5.33 GB',\n",
       " '',\n",
       " '2022-05-22 12:30:49.634583: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.',\n",
       " '2022-05-22 12:30:49.634704: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)',\n",
       " '2022-05-22 12:30:49.678078: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz',\n",
       " '2022-05-22 12:30:49.679625: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.',\n",
       " '1/3 = 33.3%, ELAPSED: 0.0s, ETA: 0.0s',\n",
       " '2022-05-22 12:30:50.682167: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.',\n",
       " '2/3 = 66.7%, ELAPSED: 3.6s, ETA: 1.8s',\n",
       " '3/3 = 100.0%, ELAPSED: 4.8s, ETA: 0.0s',\n",
       " 'Writing attention maps to /Users/lizzy/Desktop/Universita/tesi/git/dialogue_coherence/preprocessed_conversational_attn.pkl...',\n",
       " 'Done!']"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!!  python /Users/lizzy/Desktop/Universita/tesi/git/dialogue_coherence/extract_attention.py --preprocessed-data-file /Users/lizzy/Desktop/Universita/tesi/git/dialogue_coherence/preprocessed_conversational.json --bert-dir /Users/lizzy/Desktop/Universita/tesi/git/dialogue_coherence/bert/bert-base-cased-conversational"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Loading attention data',\n",
       " 'WARNING:tensorflow:From /Users/lizzy/Desktop/Universita/tesi/git/dialogue_coherence/venv/lib/python3.9/site-packages/tensorflow/python/compat/v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.',\n",
       " 'Instructions for updating:',\n",
       " 'non-resource variables are not supported in the long term',\n",
       " 'Computing head distances',\n",
       " '1/40 = 2.5%, ELAPSED: 0.0s, ETA: 0.0s',\n",
       " '2/40 = 5.0%, ELAPSED: 0.6s, ETA: 11.0s',\n",
       " '3/40 = 7.5%, ELAPSED: 13.1s, ETA: 161.8s',\n",
       " '4/40 = 10.0%, ELAPSED: 18.6s, ETA: 167.6s',\n",
       " '5/40 = 12.5%, ELAPSED: 23.6s, ETA: 165.5s',\n",
       " '6/40 = 15.0%, ELAPSED: 39.0s, ETA: 221.2s',\n",
       " '7/40 = 17.5%, ELAPSED: 39.6s, ETA: 186.8s',\n",
       " '8/40 = 20.0%, ELAPSED: 46.4s, ETA: 185.7s',\n",
       " '9/40 = 22.5%, ELAPSED: 48.9s, ETA: 168.4s',\n",
       " '10/40 = 25.0%, ELAPSED: 62.2s, ETA: 186.6s',\n",
       " '11/40 = 27.5%, ELAPSED: 64.8s, ETA: 171.0s',\n",
       " '12/40 = 30.0%, ELAPSED: 84.3s, ETA: 196.7s',\n",
       " '13/40 = 32.5%, ELAPSED: 84.6s, ETA: 175.8s',\n",
       " '14/40 = 35.0%, ELAPSED: 108.5s, ETA: 201.5s',\n",
       " '15/40 = 37.5%, ELAPSED: 113.5s, ETA: 189.2s',\n",
       " '16/40 = 40.0%, ELAPSED: 118.1s, ETA: 177.2s',\n",
       " '17/40 = 42.5%, ELAPSED: 118.8s, ETA: 160.7s',\n",
       " '18/40 = 45.0%, ELAPSED: 127.0s, ETA: 155.2s',\n",
       " '19/40 = 47.5%, ELAPSED: 160.7s, ETA: 177.6s',\n",
       " '20/40 = 50.0%, ELAPSED: 165.9s, ETA: 165.9s',\n",
       " '21/40 = 52.5%, ELAPSED: 169.1s, ETA: 153.0s',\n",
       " '22/40 = 55.0%, ELAPSED: 172.1s, ETA: 140.8s',\n",
       " '23/40 = 57.5%, ELAPSED: 175.1s, ETA: 129.4s',\n",
       " '24/40 = 60.0%, ELAPSED: 179.4s, ETA: 119.6s',\n",
       " '25/40 = 62.5%, ELAPSED: 179.6s, ETA: 107.8s',\n",
       " '26/40 = 65.0%, ELAPSED: 180.0s, ETA: 96.9s',\n",
       " '27/40 = 67.5%, ELAPSED: 180.9s, ETA: 87.1s',\n",
       " '28/40 = 70.0%, ELAPSED: 181.4s, ETA: 77.7s',\n",
       " '29/40 = 72.5%, ELAPSED: 185.7s, ETA: 70.4s',\n",
       " '30/40 = 75.0%, ELAPSED: 187.6s, ETA: 62.5s',\n",
       " '31/40 = 77.5%, ELAPSED: 188.6s, ETA: 54.8s',\n",
       " '32/40 = 80.0%, ELAPSED: 191.3s, ETA: 47.8s',\n",
       " '33/40 = 82.5%, ELAPSED: 221.3s, ETA: 46.9s',\n",
       " '34/40 = 85.0%, ELAPSED: 238.9s, ETA: 42.2s',\n",
       " '35/40 = 87.5%, ELAPSED: 239.7s, ETA: 34.2s',\n",
       " '36/40 = 90.0%, ELAPSED: 241.7s, ETA: 26.9s',\n",
       " '37/40 = 92.5%, ELAPSED: 249.1s, ETA: 20.2s',\n",
       " '38/40 = 95.0%, ELAPSED: 249.7s, ETA: 13.1s',\n",
       " '39/40 = 97.5%, ELAPSED: 288.0s, ETA: 7.4s',\n",
       " '40/40 = 100.0%, ELAPSED: 310.4s, ETA: 0.0s']"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!! python head_distances.py --attn-data-file /Users/lizzy/Desktop/Universita/tesi/git/dialogue_coherence/preprocessed_conversational_attn.pkl --outfile /Users/lizzy/Desktop/Universita/tesi/git/dialogue_coherence/head_distances_conversational.pkl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# bert-base-uncased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lizzy/Desktop/Universita/tesi/git/dialogue_coherence/venv/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model_class, tokenizer_class, pretrained_weights = (ppb.BertModel, ppb.BertTokenizer, 'bert-base-uncased')\n",
    "tokenizer = tokenizer_class.from_pretrained(pretrained_weights)\n",
    "model = model_class.from_pretrained(pretrained_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dialoghi interi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13118/13118 [00:05<00:00, 2288.96it/s]\n"
     ]
    }
   ],
   "source": [
    "lst_dict = []\n",
    "with open('/Users/lizzy/Desktop/Universita/tesi/git/dialogue_coherence/data/EMNLP_dataset/dialogues_text.txt', 'r') as f:\n",
    "    lines = f.readlines()\n",
    "    for l in tqdm(lines):\n",
    "        tokenized = tokenizer.encode(l.replace(\"__eou__\", \"[SEP]\"))[:-1]\n",
    "        if len(tokenized) < 513:\n",
    "            lst_dict.append({'tokens': tokenizer.convert_ids_to_tokens(tokenized), \n",
    "                             'words': [\"[CLS]\"] + l.replace(\"__eou__\", \"[SEP]\").replace(\"'\", \" ' \").split(), \n",
    "                             'text': l.replace(\"__eou__\", \"\").replace(\"\\n\", \"\")})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Coppie di utterance nel dialogo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13118/13118 [04:54<00:00, 44.48it/s] \n"
     ]
    }
   ],
   "source": [
    "lst_dict = []\n",
    "with open('/Users/lizzy/Desktop/Universita/tesi/git/dialogue_coherence/data/EMNLP_dataset/dialogues_text.txt', 'r') as f:\n",
    "    lines = f.readlines()\n",
    "    for l in tqdm(lines):\n",
    "        utt_lst = l.split(\"__eou__\")\n",
    "        for i, utt1 in enumerate(utt_lst):\n",
    "            for utt2 in utt_lst[i:]:\n",
    "                tokenized = tokenizer.encode(utt1+\" [SEP] \"+utt2)\n",
    "                utt1 = utt1.replace(\"'\", \" ' \").replace(\"\\n\", \"\")\n",
    "                utt2 = utt2.replace(\"'\", \" ' \").replace(\"\\n\", \"\")\n",
    "                lst_dict.append({'tokens': tokenizer.convert_ids_to_tokens(tokenized), \n",
    "                                    'words': [\"[CLS]\"] + utt1.split() + [\"[SEP]\"] + utt2.split() + [\"[SEP]\"], \n",
    "                                    'text': utt1 + \" \" + utt2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str_to_save = json.dumps(lst_dict[:50])\n",
    "with open('/Users/lizzy/Desktop/Universita/tesi/git/dialogue_coherence/preprocessed.json', 'w') as f:\n",
    "       f.write(str_to_save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Creating examples...',\n",
       " 'WARNING:tensorflow:From /Users/lizzy/Desktop/Universita/tesi/git/dialogue_coherence/venv/lib/python3.9/site-packages/tensorflow/python/compat/v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.',\n",
       " 'Instructions for updating:',\n",
       " 'non-resource variables are not supported in the long term',\n",
       " 'Traceback (most recent call last):',\n",
       " '  File \"/Users/lizzy/Desktop/Universita/tesi/git/dialogue_coherence/extract_attention.py\", line 146, in <module>',\n",
       " '    main()',\n",
       " '  File \"/Users/lizzy/Desktop/Universita/tesi/git/dialogue_coherence/extract_attention.py\", line 109, in main',\n",
       " '    example = Example(features, tokenizer, args.max_sequence_length)',\n",
       " '  File \"/Users/lizzy/Desktop/Universita/tesi/git/dialogue_coherence/extract_attention.py\", line 29, in __init__',\n",
       " '    self.input_ids = tokenizer.convert_tokens_to_ids(self.tokens)',\n",
       " '  File \"/Users/lizzy/Desktop/Universita/tesi/git/dialogue_coherence/bert/tokenization.py\", line 184, in convert_tokens_to_ids',\n",
       " '    return convert_by_vocab(self.vocab, tokens)',\n",
       " '  File \"/Users/lizzy/Desktop/Universita/tesi/git/dialogue_coherence/bert/tokenization.py\", line 145, in convert_by_vocab',\n",
       " '    output.append(vocab[item])',\n",
       " \"KeyError: 'stink'\"]"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!!  python /Users/lizzy/Desktop/Universita/tesi/git/dialogue_coherence/extract_attention.py --preprocessed-data-file /Users/lizzy/Desktop/Universita/tesi/git/dialogue_coherence/preprocessed.json --bert-dir /Users/lizzy/Desktop/Universita/tesi/git/dialogue_coherence/bert/bert-base-cased-conversational"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Loading attention data',\n",
       " 'WARNING:tensorflow:From /Users/lizzy/Desktop/Universita/tesi/git/dialogue_coherence/venv/lib/python3.9/site-packages/tensorflow/python/compat/v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.',\n",
       " 'Instructions for updating:',\n",
       " 'non-resource variables are not supported in the long term',\n",
       " 'Computing head distances',\n",
       " '1/49 = 2.0%, ELAPSED: 0.0s, ETA: 0.0s',\n",
       " '2/49 = 4.1%, ELAPSED: 0.4s, ETA: 8.9s',\n",
       " '3/49 = 6.1%, ELAPSED: 1.0s, ETA: 14.6s',\n",
       " '4/49 = 8.2%, ELAPSED: 1.1s, ETA: 12.3s',\n",
       " '5/49 = 10.2%, ELAPSED: 1.9s, ETA: 16.9s',\n",
       " '6/49 = 12.2%, ELAPSED: 2.2s, ETA: 15.7s',\n",
       " '7/49 = 14.3%, ELAPSED: 2.2s, ETA: 13.3s',\n",
       " '8/49 = 16.3%, ELAPSED: 3.6s, ETA: 18.7s',\n",
       " '9/49 = 18.4%, ELAPSED: 5.3s, ETA: 23.7s',\n",
       " '10/49 = 20.4%, ELAPSED: 7.3s, ETA: 28.5s',\n",
       " '11/49 = 22.4%, ELAPSED: 9.4s, ETA: 32.4s',\n",
       " '12/49 = 24.5%, ELAPSED: 10.3s, ETA: 31.7s',\n",
       " '13/49 = 26.5%, ELAPSED: 10.7s, ETA: 29.7s',\n",
       " '14/49 = 28.6%, ELAPSED: 12.7s, ETA: 31.7s',\n",
       " '15/49 = 30.6%, ELAPSED: 14.9s, ETA: 33.8s',\n",
       " '16/49 = 32.7%, ELAPSED: 17.3s, ETA: 35.6s',\n",
       " '17/49 = 34.7%, ELAPSED: 18.4s, ETA: 34.6s',\n",
       " '18/49 = 36.7%, ELAPSED: 19.0s, ETA: 32.7s',\n",
       " '19/49 = 38.8%, ELAPSED: 21.5s, ETA: 34.0s',\n",
       " '20/49 = 40.8%, ELAPSED: 24.4s, ETA: 35.4s',\n",
       " '21/49 = 42.9%, ELAPSED: 25.7s, ETA: 34.3s',\n",
       " '22/49 = 44.9%, ELAPSED: 26.5s, ETA: 32.5s',\n",
       " '23/49 = 46.9%, ELAPSED: 29.3s, ETA: 33.1s',\n",
       " '24/49 = 49.0%, ELAPSED: 30.8s, ETA: 32.0s',\n",
       " '25/49 = 51.0%, ELAPSED: 31.6s, ETA: 30.3s',\n",
       " '26/49 = 53.1%, ELAPSED: 32.1s, ETA: 28.4s',\n",
       " '27/49 = 55.1%, ELAPSED: 32.3s, ETA: 26.3s',\n",
       " '28/49 = 57.1%, ELAPSED: 32.3s, ETA: 24.2s',\n",
       " '29/49 = 59.2%, ELAPSED: 33.7s, ETA: 23.3s',\n",
       " '30/49 = 61.2%, ELAPSED: 52.5s, ETA: 33.2s',\n",
       " '31/49 = 63.3%, ELAPSED: 59.9s, ETA: 34.8s',\n",
       " '32/49 = 65.3%, ELAPSED: 63.1s, ETA: 33.5s',\n",
       " '33/49 = 67.3%, ELAPSED: 63.5s, ETA: 30.8s',\n",
       " '34/49 = 69.4%, ELAPSED: 98.5s, ETA: 43.5s',\n",
       " '35/49 = 71.4%, ELAPSED: 122.9s, ETA: 49.2s',\n",
       " '36/49 = 73.5%, ELAPSED: 137.2s, ETA: 49.5s',\n",
       " '37/49 = 75.5%, ELAPSED: 155.9s, ETA: 50.6s',\n",
       " '38/49 = 77.6%, ELAPSED: 167.2s, ETA: 48.4s',\n",
       " '39/49 = 79.6%, ELAPSED: 171.9s, ETA: 44.1s',\n",
       " '40/49 = 81.6%, ELAPSED: 177.5s, ETA: 39.9s',\n",
       " '41/49 = 83.7%, ELAPSED: 179.1s, ETA: 34.9s',\n",
       " '42/49 = 85.7%, ELAPSED: 179.1s, ETA: 29.9s',\n",
       " '43/49 = 87.8%, ELAPSED: 179.8s, ETA: 25.1s',\n",
       " '44/49 = 89.8%, ELAPSED: 180.6s, ETA: 20.5s',\n",
       " '45/49 = 91.8%, ELAPSED: 182.6s, ETA: 16.2s',\n",
       " '46/49 = 93.9%, ELAPSED: 183.3s, ETA: 12.0s',\n",
       " '47/49 = 95.9%, ELAPSED: 183.5s, ETA: 7.8s',\n",
       " '48/49 = 98.0%, ELAPSED: 184.4s, ETA: 3.8s',\n",
       " '49/49 = 100.0%, ELAPSED: 186.6s, ETA: 0.0s']"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!! python head_distances.py --attn-data-file /Users/lizzy/Desktop/Universita/tesi/git/dialogue_coherence/preprocessed_attn.pkl --outfile /Users/lizzy/Desktop/Universita/tesi/git/dialogue_coherence/head_distances.pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c4296dd8706fac5325e6d01948524d9b7ee219aac805fc0ef4b567dd0caece99"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
